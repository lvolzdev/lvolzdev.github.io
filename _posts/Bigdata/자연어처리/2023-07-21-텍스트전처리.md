---
title: "[NLP] 텍스트전처리 Text Preprocessing"
date: 2023-07-20 20:00:00 +09:00
categories: [Bigdata, 자연어처리]
tags: [NLP, Python]
---

텍스트 전처리 과정을 소개하고 영어와 한국어에서 적용하는 법을 작성했습니다.

## 목차
- [1. Text Preprocessing Overview](#1-text-preprocessing-overview)
  * [Tokenization 토큰화](#tokenization-토큰화)
  * [Cleaning 정제](#cleaning-정제)
  * [Normalization 정규화](#normalization-정규화)
  * [Lemmatization 표제어 추출](#lemmatization-표제어-추출)
  * [Stemming 어간 추출](#stemming-어간-추출)
  * [Stemming vs Lemmatization**](#stemming-vs-lemmatization)
  * [Stopword Removal 불용어 제거](#stopword-removal-불용어-제거)
- [2. Text Preprocessing for English](#2-text-preprocessing-for-english)
  * [Tokenization](#tokenization)
  * [Lemmatization](#lemmatization)
  * [Stopword removal](#stopword-removal)
- [3. Text Preprocessing for Korean](#3-text-preprocessing-for-korean)
  * [Tokenization](#tokenization-1)
  * [Cleaning](#cleaning)
  * [Normalization](#normalization)
  * [Lemmatization & Stemming](#lemmatization--stemming)



# 1. Text Preprocessing Overview

**텍스트 전처리 과정**
- Tokenization - 토큰이라는 단위로 나누기
- POS Tagging - 토큰에 적합한 품사 태깅
- Normalization - 표현 방법이 다른 단어들 `통합`하여 같은 언어로 만들기
- Cleaning - 노이즈 제거
- Stopword Removal - 무의미한 단어 토큰 제거
- Lemmatization - 기본 사전형 단어인 `표제어 추출`
- Stemming - 규칙 기반으로 단어를 원형으로 복원

## Tokenization 토큰화
Corpus(말뭉치) → Token(의미 있는 단위) 으로 나누기. 보통 문장, 단어 단위로.

Pos Tagging - 토큰에 앞뒤 문맥상 적합한 품사 태깅

## Cleaning 정제
Corpus에서 노이즈 데이터 제거, Tokenization 이후, 이전 진행  
- 등장 빈도 적은 애, 길이 짧은 애(eng), 불용어(stopword) 제거
- 오탈자, 띄어쓰기 교정

## Normalization 정규화
같은 의민데 다른 표현으로 쓰인 단어들 하나로 통합, 같은 단어로 만들어주기
- 대소문자 통합, Hmm, Hmmmmmm → Hmm 통일

## Lemmatization 표제어 추출
형변환 `사전 기반`으로 `표제어 추출`  

대상 단어의 품사에 맞는 단어의 표제어 추출 (in eng)  

- is, am, are → be
- cooking(v) → cook
- cooking(n) → cooking(n)

## Stemming 어간 추출

규칙 기반으로 단어의 변형된 형태를 제거/치환하여 어간 추출 := 쉽게 말하면 정해진 규칙으로 `어미 잘라내기!`  

Stemming 결과가 존재하지 않는 단어일 수 있음.  

- 형태학적 Parsing - 어간, 접사 분리하는 과정.
    - 형태소(morpheme) - 의미가 있는 가장 작은 말의 단위
        - 어간(stem) - 단어 의미 담고 있는, 핵심 부분
        - 접사(affix) - 추가 의미

## Stemming vs Lemmatization**

stemming - 단어같지 않은 결과 다수. / 빠르고 간단.

lemmatization - 정말 그 단어의 원형 추출. / 복잡. 단어 사전에서 찾고 ..

![img](/assets/img/nlp-stemming-vs-lemmatization.png)

## Stopword Removal 불용어 제거

무의미한 단어 토큰 == 불용어 제거

- i, my, me, ~를, ~을 등등

---

# 2. Text Preprocessing for English

## Tokenization

Sentence Tokenization - NLTK sent_tokenization 사용

```python
sent_tokens = nltk.sent_tokenize(my_str)
```

Word Tokenization - NLTK word_tokenize, WordPuctTokenizer, RegexTokenizer 사용

```python
result = nltk.word_tokenize(my_str)
 or
result = nltk.wordpunct_tokenize(my_str)
```

## Lemmatization 
- NLTK WordNetLemmatizezr 사용  
대상 단어의 품사에 맞는 단어의 표제어 추출

```python
lemmatizer = WordNetLemmatizer()

print([lemmatizer.lemmatize(word) for word in results])
```

## Stopword removal
- NLTK에서 미리 정한 stopword list 존재, nltk.corpus.stopwords
- 사용자가 직접 추가도 가능

```python
stop = stopwords.word('english')
stop.extend(['hey', 'amazon'])
```

---

# 3. Text Preprocessing for Korean

## Tokenization
Sentence Tokenization  - KSS split_sentence 사용  

Word Tokenization - 영어와 다르게 띄어쓰기로 단어 토큰이 구분되지 X (띄어쓰기로 구분 == 어절)

<br>

**한국어는 교착어임.**  
교착어: 어간에 조사, 어미 등의 문법 형태소가 결합하여 문법적인 기능이 부여되는 언어.  
ex. 사과가, 사과에, 사과를, 사과 … → 띄어쓰기 단위로 토큰화하면 모두 다른 단어로 간주됨 헐!

따라서, 한국어 토큰화에는 반드시 `형태소 분석` 필요. (뜻 O)

- 자립 형태소 - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등
- 의존 형태소 - 접사, 어미, 조사, 어간

한국어 토크나이저 (`형태소 분석기`) - task에 맞게 사용, Pos tagging 역할까지!

<br>

- 지도 학습 (Supervised learning) 기반
    - 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 학습된.
        - KoNLPy의 5개 형태소 분석기
            - Mecab, KOMORAN, Hannanum, Khaiii, 꼬꼬마
        - 카카오 Khaiii 형태소 분석기
- 비지도 학습 (Unsupervised learning) 기반
    - 데이터의 패턴을 모델 스스로 학습하게. (자주 등장한 단어를 형태소로 인식)
        - Soynlp, ssentencepiece 등

## Cleaning
오탈자 교정/제거 
- 네이버/카카오 api, py-hanspell, 부산대 맞춤법 검사기(👍)

❓❓❓ **py-hanspell 사용 안 됨. → json 오류**

- https://github.com/ssut/py-hanspell/issues/31

```python
from hanspell import spell_checker

sent = '맞춤법 틀리면 외 않되?'
spelled_sent = spell_checker.check(sent)

hanspell_sent = spelled_sent.checked
print(hanspell_sent)
```

띄어쓰기 교정
- KoSpacing

## Normalization
- soynlp의 normalizer 사용
    - Hmm, Hmmmm, Hmmmmmmm → Hmm

```python
from soynlp.normalizer import *

print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))
# output : 아ㅋㅋ영화존잼쓰ㅠㅠ
```

## Lemmatization & Stemming
대상 단어의 품사에 맞는 단어의 표제어/어간 추출하는 행위  
표제어 lemma : 기본 사전형 단어

형태소 분석기에서 기본적인 표제어 추출, 어간 추출은 할 수 있는데 성능 구림!!!  
→ why? 한국어는 5언 9품사 .. 가변어가 다양하게 활용돼서 어간 추출하기 어렵 ..


## 용언의 활용 conjugation
- 용언의 어간(stem)이 어미(ending)을 가지는 일.
- 가변어(용언=동사+형용사)는 어간에 결합되는 어미가 달라지면서 표현형이 바뀜!
    - 원형 Canonical form, lemma
        - 어간에 어미 ‘-다’ 결합된 형태. ex. 하 + 다 / 가 + 다
    - 표현형 Surficial form
        - 그 외의 어미가 결합된 형태. ex. 하 + ㄴ다 → 한다

용언의 활용 1. 규칙 활용  
: 자음/모음열 기준에서 변화 X 활용

- 1) 두 단어가 그대로 결합되는 경우. ex. 잡다: 잡/어간 + 다/어미
- 2) 어간 마지막 글자에 종성 없는 경우 받침 추가. ex. 말한다: 말하/어간 + ㄴ다/어미


용언의 활용 2. 불규칙 활용  
: 자음/모음열 기준에서 변화 O 활용

- 1) 어간 마지막 종성 ‘ㄷ’ + 어미 첫글자 ‘ㅇ’ : ‘ㄷ’ → ‘ㄹ’
    - ex. 깨달아: 깨닫/어간 + 아/어미
    
    2) 어간 마지막 종성 ‘ㅂ’ + 어미 첫글자 ‘ㅇ’ : ‘ㅂ’ → ‘ㅗ/ㅜ’
    
    - ex. 더러워: 더럽/어간 + 워/어미
    - ex. 아름다워: 아름답/어간 + 아/어미
    
    3) 많음 ..
    
 <br>   

정리!  
- **활용 Conjugation** - 용언의 원형 → 표현형으로 변환하는 과정
- **표제어 추출 lemmatiztion** - 용언의 표현형 → 원형으로 변환하는 과정 !!!
- **한국어는 다양한 활용으로 인해 lemmatization이 어렵다!**